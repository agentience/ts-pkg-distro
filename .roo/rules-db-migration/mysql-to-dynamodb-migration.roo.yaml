title: "MySQL to DynamoDB Migration Best Practices"
description: "Guidelines and best practices for migrating data from MySQL to AWS DynamoDB using AWS Database Migration Service (DMS)"

migration_strategy:
  tool: "AWS Database Migration Service (DMS)"
  type: "Full Load + CDC (Change Data Capture)"
  benefits:
    - "Minimizes downtime during migration"

data_modeling_strategies:
  description: "Key strategies for modeling data in DynamoDB when migrating from a relational database."
  single_table:
    name: "Single-Table Design"
    description: "Consolidating multiple relational tables into a single DynamoDB table. This is often recommended for related data that is frequently accessed together, optimizing for read performance and reducing the number of requests."
    considerations:
      - "Requires careful design of Partition Keys and Sort Keys to support various access patterns."
      - "Can lead to complex item structures and query logic."
      - "Best suited for data with well-defined access patterns."
  table_per_model:
    name: "Table Per Model"
    description: "Creating a separate DynamoDB table for each distinct entity or model from the relational database. This is simpler to implement initially but can lead to more complex queries for related data that spans multiple tables."
    considerations:
      - "Easier to map directly from relational schema."
      - "Requires multiple requests to retrieve related data."
      - "May be suitable for data with few or simple relationships."
  hybrid:
    name: "Hybrid Approach"
    description: "Combining single-table design for highly related data with table-per-model for less frequently accessed or less related data. This approach balances the benefits of both strategies based on the specific access patterns and relationship complexity."
    considerations:
      - "Requires careful analysis of data relationships and access patterns."
      - "Can offer a good balance between simplicity and performance."
      - "May involve more complex data modeling decisions."
  relationship_complexity:
    description: "Choosing the appropriate strategy depends on the complexity of relationships in the relational database and the application's access patterns."
    guidelines:
      - "Highly normalized relational schemas with complex relationships often benefit from a single-table design to reduce joins."
      - "Schemas with simple or few relationships might be adequately served by a table-per-model approach."
      - "Analyze access patterns first â€“ DynamoDB modeling is access pattern driven, not relationship driven."
      - "Consider using Global Secondary Indexes (GSIs) and Local Secondary Indexes (LSIs) to support different query patterns within a single table."

prerequisites:
  mysql_source:
    - "Enable binary logging for CDC."
    - "Configure appropriate binary log retention based on full load time."
    - "Grant AWS DMS user necessary permissions (REPLICATION CLIENT, REPLICATION SLAVE, SELECT)."
    - "Configure network connectivity and security groups to allow access from DMS replication instance."
  dynamodb_target:
    - "Pre-create the target table with appropriate partition and sort keys based on access patterns."
    - "Consider using On-Demand capacity during migration and adjust later based on traffic."
    - "Create an IAM role for DMS with permissions to interact with DynamoDB tables and control tables (awsdms_apply_exceptions, awsdms_full_load_exceptions)."
  replication_instance:
    - "Choose an appropriate instance size and storage based on data volume and performance requirements."
    - "Use Multi-AZ deployment for high availability."
    - "Avoid making the instance publicly accessible unless necessary for source/target outside VPC."

dms_task_configuration:
  general:
    - "Use object mapping ('map-record-to-record') to handle schema differences and composite primary keys."
    - "Enable CloudWatch Logs for troubleshooting and monitoring."
    - "Set target table preparation mode to 'Do nothing' if pre-creating the table."
    - "Disable LOB settings if the source table doesn't contain large objects."
  performance_tuning:
    - "Adjust ParallelLoadThreads and ParallelLoadBufferSize for full load performance, monitoring for throttling on the target."
    - "Adjust commit rate during full load for better performance."
    - "Consider distributing workload using multiple tasks with source filter conditions for very large tables."

scripted_migration_process:
  description: "Guidelines for implementing a custom scripted migration process using TypeScript as an alternative to AWS DMS."
  when_to_use:
    - "When AWS DMS limitations or specific requirements necessitate a custom approach."
    - "For fine-grained control over the migration logic and data transformation."
    - "When migrating smaller datasets where the overhead of setting up DMS might be disproportionate."
  key_steps:
    - name: "Connect to Databases"
      description: "Establish connections to both the source MySQL database (using a library like `mysql2`) and the target DynamoDB (using the AWS SDK for JavaScript/TypeScript)."
    - name: "Read Data from Source"
      description: "Query data from the MySQL database, potentially in batches to manage memory usage for large tables."
    - name: "Transform Data"
      description: "Implement logic to transform the relational data structure into the desired DynamoDB item structure, applying the chosen data modeling strategy (single-table, table-per-model, or hybrid)."
    - name: "Write Data to Target"
      description: "Write the transformed data to the target DynamoDB table(s) using the AWS SDK. Consider using batch write operations for efficiency."
    - name: "Handle Relationships"
      description: "Implement logic to handle relationships between entities based on the chosen data modeling strategy. This might involve denormalizing data, using secondary indexes, or performing multiple queries."
  error_handling:
    - "Implement robust error handling for database connections, read/write operations, and data transformation."
    - "Log errors with sufficient detail for debugging."
    - "Consider retry mechanisms for transient errors (e.g., DynamoDB throttling)."
  monitoring:
    - "Implement logging to track the progress of the migration (e.g., number of records processed, tables completed)."
    - "Monitor resource utilization on the source and target databases during the migration."
    - "Consider using AWS CloudWatch for monitoring script execution and resource metrics."
  considerations:
    - "Managing state and progress for large migrations to allow for resumption if interrupted."
    - "Implementing change data capture (CDC) if ongoing replication is required (more complex than with DMS)."
    - "Performance optimization for reading from MySQL and writing to DynamoDB."
    - "Security of database credentials and network access."

monitoring_and_validation:
  - "Continuously monitor CloudWatch metrics (compute, memory, network, IOPS) for the replication instance and target DynamoDB table."
  - "Monitor for DynamoDB throttling events and adjust ParallelLoadThreads or capacity mode."
  - "Optionally validate data migration by querying the target DynamoDB table (be cautious with scans on large tables)."

cleanup:
  - "Delete created cloud resources (replication instance, endpoints, task, etc.) after successful migration."

considerations:
  - "Schema design differences between relational (MySQL) and NoSQL (DynamoDB) databases."
  - "Impact of partition and sort key selection on DynamoDB performance and CDC operations."
  - "Handling of unsupported data types."
  - "Workload on the source database during full load."